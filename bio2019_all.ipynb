{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input:\n",
    "- features: (samples x 32830/925032 gene/probeset) RNAseq counts (+extracted features); data has been batch and count normalized\n",
    "- class: (367 train sample) gestational age 8-42 weeks\n",
    "\n",
    "## output:\n",
    "- class: (368 test sample) gestational age 8-42 weeks rounded to 1 decimal place\n",
    "\n",
    "# 0: preliminaries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list=ls(all=T)) # clean the environment\n",
    "set.seed(10)\n",
    "\n",
    "## root \n",
    "#root = \"/mnt/f/Brinkman group/current/Alice/ai4all2019_bio\"\n",
    "root = \"C:\\\\Users\\\\raque\\\\Documents\\\\GitHub\\\\ai4all2019_bio\"\n",
    "setwd(root)\n",
    "\n",
    "## directories\n",
    "input_dir = paste0(root,\"/00_input\") # raw data directory\n",
    "feat_dir = paste0(root,\"/01_features\") # feature directory\n",
    "model_dir = paste0(root, \"/02_models\") # model directory\n",
    "result_dir = paste0(root, \"/03_results\") # stats/plots directory\n",
    "sapply(c(input_dir,feat_dir, model_dir, result_dir), \n",
    "       function(x) dir.create(x, showWarnings=F))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "\n",
    "## load packages; need to fix according to what model we'll be using\n",
    "pkgs = c(\"Rfast\", \"stringr\", \"plyr\", \"dplyr\", \"Matrix\", # var, str_, llply, etc\n",
    "         \"lattice\", \"ggplot2\", # barplot, plots\n",
    "         \"foreach\", \"doMC\", # parallel back-end\n",
    "         \"caret\", \"e1071\", \"ranger\", \"ANN2\", \"randomForest\",\n",
    "         \"elasticnet\", \"fastICA\", \"foba\", \"glmnet\",\"kernlab\", \n",
    "         \"KRLS\", \"lars\", \"leaps\", \"nnls\", \"nodeHarvest\", \n",
    "         \"partDSA\", \"pls\", \"plsRglm\", \"rpart\", \"rqPen\",\n",
    "         \"RSNNS\", \"spikeslab\", \"xgboost\") # ml\n",
    "pkgs_ui = setdiff(pkgs, rownames(installed.packages()))\n",
    "if (length(pkgs_ui) > 0) install.packages(pkgs_ui, verbose=F)\n",
    "sapply(pkgs, require, character.only=T)\n",
    "\n",
    "\n",
    "## script options\n",
    "no_cores = detectCores()-1 # number of cores to use in parallel\n",
    "registerDoMC(no_cores)\n",
    "\n",
    "overwrite = F # overwrite results?\n",
    "\n",
    "\n",
    "## load input files\n",
    "\n",
    "# sample annotation file columns:\n",
    "#  SampleID: unique identifier of the sample (matching the name of the .CEL file in HTA20 folder, except for extension .CEL);\n",
    "#  GA: gestational age as determined by the last menstrual period and or ultrasound;\n",
    "#  Batch: the batch identifier;\n",
    "#  Set: name of the source dataset;\n",
    "#  Train: 1 for samples to be used for training, 0 for samples to be used for test;\n",
    "meta = read.csv(paste0(input_dir,\"/anoSC1_v11_nokey.csv\"))\n",
    "\n",
    "# submission template\n",
    "class_final = read.csv(paste0(input_dir,\"/TeamX_SC1_prediction.csv\")) \n",
    "\n",
    "# RNASEQ data: each row is a gene and each column a patient \n",
    "#  probeset: gene ID\n",
    "#  pid: patient id \n",
    "data0 = t(get(load(paste0(input_dir,\"/HTA20_RMA.RData\"))))\n",
    "\n",
    "\n",
    "## data exploration ----------------------------\n",
    "range(data0) # range: 0.9365626 14.2836285\n",
    "gid = colnames(data0); head(gid) # genes IDS\n",
    "pid = rownames(data0); head(pid) # Patient IDS  \n",
    "\n",
    "# plot stats: mean count, pearson/spearman corr\n",
    "datavars = colVars(data0)\n",
    "meancount = colMeans(data0)\n",
    "meancounto = order(meancount)\n",
    "\n",
    "tr_ind0 = which(meta$Train==1)\n",
    "corpe = apply(data0[tr_ind0,meancounto], 2, function(x) \n",
    "  cor(x, meta$GA[tr_ind0], method=\"pearson\"))\n",
    "corpep = apply(data0[tr_ind0,meancounto], 2, function(x) \n",
    "  cor.test(x, meta$GA[tr_ind0], method=\"pearson\")$p.value)\n",
    "\n",
    "corsp = apply(data0[tr_ind0,meancounto], 2, function(x) \n",
    "  cor(x, meta$GA[tr_ind0], method=\"spearman\"))\n",
    "corspp = apply(data0[tr_ind0,meancounto], 2, function(x) \n",
    "  cor.test(x, meta$GA[tr_ind0], method=\"spearman\")$p.value)\n",
    "\n",
    "# plot stats\n",
    "png(paste0(result_dir,\"/gene_stats.png\"), width=800, height=1000)\n",
    "par(mfcol=c(4,1), mar=c(3,3,3,3))\n",
    "plot(density(data0), main=\"count distribution\")\n",
    "plot(log(meancount), datavars, pch=16, cex=.3, \n",
    "     main=\"gene ln(mean count) x variance\")\n",
    "plot(corpe, cex=1-corpep, pch=16, \n",
    "     main=\"gene (asc mean count order) x pearson corr with GA (size=1-pvalue)\")\n",
    "# plot(abs(corpe), cex=1-corpep, pch=16)\n",
    "plot(corsp, cex=1-corspp, pch=16, \n",
    "     main=\"gene (asc mean count order) x spearman corr with GA (size=1-pvalue)\")\n",
    "# plot(abs(corsp), cex=1-corspp, pch=16)\n",
    "graphics.off()\n",
    "\n",
    "\n",
    "\n",
    "#--------#--------#--------#--------#--------#--------#--------\n",
    "# 1: feature extraction ---------------------------------------\n",
    "#--------#--------#--------#--------#--------#--------#--------\n",
    "\n",
    "## temp data prep -------------------------------------------\n",
    "# save only high variance genes and those with high sig pearson corr with GA\n",
    "m = data0[,datavars>quantile(datavars,0.7) & abs(corsp)>quantile(abs(corsp),0.7) & corspp<quantile(corspp,0.1) & meancount>quantile(meancount,0.5)]\n",
    "# # rfe to reduce features random forest\n",
    "# rfe_res = rfe(m[tr_ind,], meta$GA[tr_ind], sizes=c(1:8), rfeControl=rfeControl(functions=rfFuncs, method=\"cv\", number=10))\n",
    "# print(rfe_res)\n",
    "# predictors(rfe_res)\n",
    "# plot(rfe_res, type=c(\"g\", \"o\"))\n",
    "write.csv(m, file=paste0(feat_dir,\"/features_raw.csv\"))\n",
    "\n",
    "\n",
    "## 1) Removing genes with low variance ------------------------\n",
    "eliminate = data.frame(col = c(1:dim(data0)[2]), var_gene = colVars(data0))\n",
    "# head(eliminate)\n",
    "eliminate = subset(eliminate, var_gene<quantile(eliminate$var_gene, 0.3))\n",
    "# dim(data0)\n",
    "data1 = subset(data0, select = -c(eliminate$col))\n",
    "# dim(data1)\n",
    "\n",
    "\n",
    "## 2) Removing elements with low correlation with GA ----------\n",
    "data2 = data.frame(SampleID=row.names(data1), data1)\n",
    "row.names(data2) = NULL\n",
    "# Combining the two datasets \n",
    "data2 = merge(sample[,c(1,2,5)], data2, by.x = 'SampleID', by.y = 'SampleID', all = T)\n",
    "#Using only the train dataset to make the feature importance \n",
    "data2 = subset(data2, Train == 1)\n",
    "data2 = subset(data2, select = -c(Train,SampleID))\n",
    "\n",
    "eliminate = data.frame(col = c(1:dim(data1)[2]),corr = apply(data2[,-1], 2, cor, x=data2$GA))\n",
    "eliminate$corr = abs(eliminate$corr)\n",
    "eliminate = subset(eliminate, corr<quantile(eliminate$corr,0.3))\n",
    "dim(data2)\n",
    "data2 = subset(data1, select = -c(eliminate$col))\n",
    "dim(data2)\n",
    "# write.csv(data2, paste0(feat_dir,'/features_raw.csv'), row.names=T)\n",
    "\n",
    "\n",
    "## 3) PCA -----------------------------------------------------\n",
    "# https://www.datacamp.com/community/tutorials/pca-analysis-r\n",
    "# Don't need the values to be predicted, only the features\n",
    "PrePCA = preProcess(data2,method=\"pca\")\n",
    "feat.pca = predict(PrePCA,data2)\n",
    "# PrePCA\n",
    "write.csv(feat.pca, paste0(feat_dir,'/features_pca.csv'), row.names=T)\n",
    "\n",
    "\n",
    "## 4) Random Forest --------------------------------------------\n",
    "#https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest\n",
    "#https://uc-r.github.io/random_forests\n",
    "#http://www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/\n",
    "# It depends on the features and the value to be predicted \n",
    "# data2 = as.matrix(data2)\n",
    "metric = \"Accuracy\"\n",
    "#Number randomely variable selected is mtry\n",
    "PreRF = caret::train(y=meta$GA[tr_ind0], x=data2[tr_ind0,], method='ranger',importance='impurity')\n",
    "\n",
    "# print(PreRF)\n",
    "PreRF.i = varImp(PreRF)$importance\n",
    "# PreRF.i = PreRF.i[order(PreRF.i$Overall, decreasing=T)[1:500],]\n",
    "# PreRF.i = PreRF.i[1:500,]\n",
    "\n",
    "feat.ra = data2[,order(PreRF.i, decreasing=T)[1:500]]\n",
    "write.csv(feat.ra, paste0(feat_dir,'/features_ra.csv'), row.names = T)\n",
    "\n",
    "\n",
    "## 5) Autoencoder --------------------------------------------\n",
    "#https://www.r-bloggers.com/pca-vs-autoencoders-for-dimensionality-reduction/\n",
    "#https://www.rdocumentation.org/packages/ANN2/versions/1.5/topics/autoencoder\n",
    "#https://www.rdocumentation.org/packages/ANN2/versions/1.5/topics/autoencoder\n",
    "\n",
    "#epochs changed to 100\n",
    "preA = autoencoder(data2,hidden.layers = c(1000, 500, 1000))\n",
    "feat.A = encode(preA, data2)\n",
    "write.csv(feat.A, paste0(feat_dir,'/features_a.csv'), row.names = T)\n",
    "\n",
    "# save.image()\n",
    "\n",
    "\n",
    "\n",
    "#--------#--------#--------#--------#--------#--------#--------\n",
    "# 2: regression models ----------------------------------------\n",
    "#--------#--------#--------#--------#--------#--------#--------\n",
    "\n",
    "## 0) load features ----------------------------------------\n",
    "feat_paths = list.files(feat_dir) # feature paths\n",
    "m0s = llply(feat_paths, function(xi) {\n",
    "  m0 = read.csv(paste0(feat_dir,\"/\", xi))\n",
    "  rownames(m0) = m0[,1]\n",
    "  m0 = as.matrix(m0[,-1])\n",
    "})\n",
    "names(m0s) = gsub(\".csv\",\"\",feat_paths)\n",
    "\n",
    "\n",
    "## 1) prep cvn-fold cross validation & rmse function ----------\n",
    "cvinds_path = paste0(root,\"/cvinds.Rdata\")\n",
    "if (file.exists(cvinds_path)) {\n",
    "  load(cvinds_path)\n",
    "} else {\n",
    "  cvn = 10\n",
    "  tr_ind0 = which(meta$Train==1)\n",
    "  te_ind = sample(tr_ind0, ceiling(length(tr_ind0)/11))\n",
    "  tr_ind = sample(tr_ind0[!tr_ind0%in%te_ind])\n",
    "  ctr = as.numeric(meta$GA[tr_ind])\n",
    "  cte = as.numeric(meta$GA[te_ind])\n",
    "  save(cvn,tr_ind0,te_ind,tr_ind,ctr,cte, file=cvinds_path)\n",
    "}\n",
    "# # cross validation function\n",
    "# rmse = function(x,y) sqrt(mean((x-y)^2))\n",
    "# cv_inds = split(tr_ind, cut(seq_along(tr_ind), cvn, labels=F))\n",
    "# cv_class = llply(cv_inds, function(is) meta$GA[is])\n",
    "# cv = function(data0, cv_inds, cv_class, fun, ...) {\n",
    "#   llply (1:length(cv_inds), function(i) {\n",
    "#     mtr = data0[unlist(cv_inds[-i]),]\n",
    "#     ctr = unlist(cv_class[-i])\n",
    "#     mte = data0[cv_inds[[i]],]\n",
    "#     cte_ = unlist(cv_class[-i])\n",
    "#     pte = fun(mtr, ctr, mte, ...)\n",
    "#     return(list(pte=pte, rmse=rmse(pte,cte_)))\n",
    "#   })\n",
    "# }\n",
    "# # usage:\n",
    "# result_10x_rsme_pred = cv(data0, cv_inds, cv_class, function(mtr, ctr, mte) {\n",
    "#   ...\n",
    "#   return(pred) # vector of test class prediction\n",
    "# })\n",
    "fitcv = trainControl(method=\"cv\", number=cvn)\n",
    "\n",
    "\n",
    "## 2) test regression models ---------------------------------\n",
    "\n",
    "# list models to test\n",
    "models = c(# \"ANFIS\", # takes too long; RMSE 20\n",
    "  #\"avNNet\",\"bag\",\n",
    "  # \"bagEarth\", # 8.9\n",
    "  # \"bagEarthGCV\", # 8.6; repeat\n",
    "  #\"bam\",        \"bartMachine\",\n",
    "  # \"bayesglm\", # 10\n",
    "  #\"blackboost\", \n",
    "  ## \"blasso\", # 8.4; takes a bit longer 950\n",
    "  # \"blassoAveraged\", # 8.4; repeat\n",
    "  ## \"bridge\", # 8.4; takes a bit longer 950\n",
    "  #\"brnn\",       \"BstLm\",      \"bstSm\",      \"bstTree\",   \n",
    "  #\"cforest\",    \"ctree\",      \"ctree2\",     \"cubist\",    \n",
    "  #\"DENFIS\",     \"dnn\",        \"earth\",      \"elm\",       \n",
    "  \"enet\", # 8.5      \n",
    "  # \"evtree\",     \"extraTrees\", \"FIR.DM\", # no pkg  \n",
    "  \"foba\", # 8.5      \n",
    "  # \"FS.HGD\", # 9\n",
    "  # \"gam\",        \"gamboost\", \"gamLoess\",   \"gamSpline\", # error on run\n",
    "  # \"gaussprLinear\", # 10\n",
    "  \"gaussprPoly\", # 8.3\n",
    "  \"gaussprRadial\", # 8.8\n",
    "  # \"gbm\", # 9\n",
    "  # \"gbm_h2o\",  # error on run\n",
    "  # \"gcvEarth\", # 9.5\n",
    "  # \"GFS.FR.MOGUL\",\"GFS.LT.RS\",  \"GFS.THRIFT\", # takes too long\n",
    "  # \"glm\", \"glm.nb\", # 10\n",
    "  # \"glmboost\",  # error on run\n",
    "  \"glmnet\", # 8.5\n",
    "  # \"glmnet_h2o\",  # error on run\n",
    "  # \"glmStepAIC\", # 10\n",
    "  # \"HYFIS\", # takes too long\n",
    "  \"icr\", # 8.4\n",
    "  \"kernelpls\", # 8.5\n",
    "  # \"kknn\",# 9.6       \n",
    "  # \"knn\", # 9       \n",
    "  ## \"krlsPoly\", # 8.5; takes a bit longer 900\n",
    "  \"krlsRadial\", # 8.5\n",
    "  # \"lars\", # 8.5       \n",
    "  \"lars2\", # 8.5      \n",
    "  \"lasso\", # 8.7     \n",
    "  \"leapBackward\", \"leapForward\", \"leapSeq\", # 8.5\n",
    "  # \"lm\",\"lmStepAIC\", # 10\n",
    "  # \"logicBag\",   \"logreg\", \"M5\",\"M5Rules\", # no pkg\n",
    "  # \"mlp\", # 10\n",
    "  # \"mlpKerasDecay\",\"mlpKerasDropout\",  # error on run\n",
    "  # \"mlpSGD\", # error on run\n",
    "  # \"mlpWeightDecay\", # 10 \n",
    "  # \"mlpWeightDecayML\", # 9 \n",
    "  # \"monmlp\", # 10\n",
    "  # \"msaenet\", # 10   \n",
    "  # \"mxnet\",      \"mxnetAdam\", # no pkg\n",
    "  # \"neuralnet\", # error on run\n",
    "  # \"nnet\", # 26\n",
    "  \"nnls\", # 8.5\n",
    "  \"null\", # 8.5\n",
    "  # \"parRF\", # 9      \n",
    "  \"partDSA\", # 8.5\n",
    "  # \"pcaNNet\", # 26   \n",
    "  # \"pcr\", # 8.5\n",
    "  # \"penalized\", # 9\n",
    "  \"pls\",        \"plsRglm\",    \n",
    "  # \"ppr\", \"qrf\", # 11     \n",
    "  # \"qrnn\", \"randomGLM\", # takes too long\n",
    "  # \"ranger\", # 9   \n",
    "  \"rbf\", # 8.4       \n",
    "  # \"rbfDDA\", # 27\n",
    "  # \"Rborist\", # takes too long\n",
    "  # \"relaxo\", # 8.7\n",
    "  # \"rf\", #9.7\n",
    "  # \"rfRules\", # error on run\n",
    "  # \"ridge\", # 9.5\n",
    "  # \"rlm\", # error on run       \n",
    "  \"rpart\", # 8.4     \n",
    "  # \"rpart1SE\", # 11  # \"rpart2\", # 8.6    \n",
    "  \"rqlasso\", # 8.4\n",
    "  # \"RRF\",        \"RRFglobal\", # 9 \n",
    "  # \"rvmLinear\", # 8.6\n",
    "  \"rvmPoly\",    \"rvmRadial\", # 8.5\n",
    "  # \"SBC\", # 12        \n",
    "  \"simpls\", \"spikeslab\",  # \"spls\", # 8.5; spls takes a bit longer 950\n",
    "  # \"superpc\", # lowest score 27\n",
    "  # \"svmBoundrangeString\",\n",
    "  # \"svmExpoString\", # error on run\n",
    "  # \"svmLinear\",  \"svmLinear2\", # 11 # \"svmLinear3\", # 9\n",
    "  \"svmPoly\",    \"svmRadial\",  \"svmRadialCost\",\"svmRadialSigma\", # 8.5\n",
    "  # \"svmSpectrumString\", # error on run\n",
    "  # \"treebag\", # 9.3   \n",
    "  \"widekernelpls\", # 8.5\n",
    "  # \"WM\", # 11\n",
    "  \"rqnc\", # 8.4\n",
    "  \"nodeHarvest\", # 8.6\n",
    "  \"mlpML\", # 8.5\n",
    "  \"xgbDART\" # extreme gradient boosting is good; \"xgbLinear\", # takes too long\n",
    "  # \"xgbTree\",    \"xyf\"\n",
    ")\n",
    "# models = unique(modelLookup()[modelLookup()$forReg,c(1)])\n",
    "\n",
    "# list model parameters to test\n",
    "pars = list(\n",
    "  # gbm=expand.grid(interaction.depth = c(1:5), #3 # gradient booted machine\n",
    "  #                 n.trees = (seq(1,30,3))*50,\n",
    "  #                 shrinkage = 0.1,\n",
    "  #                 n.minobsinnode = 20),\n",
    "  # ANFIS=expand.grid(max.iter=c(10,30,60,100)), # adaptive-network-based fuzzy inference system\n",
    "  # avNNet=expand.grid(size=c(50,100), decay=10^runif(5, min = -5, 1), bag=T),\n",
    "  #brnn=expand.grid(neurons=c(50,100)),\n",
    "  enet=expand.grid(lambda=10^runif(5, min=-5, 1), fraction=runif(5, min=0, max=1)),\n",
    "  # neuralnet=expand.grid(layer1=c(50,100),layer2=c(50,100),layer3=c(50,100)),\n",
    "  # blackboost=expand.grid(maxdepth=c(1,3,6,10)),\n",
    "  # xgbDART=expand.grid(nrounds, max_depth, eta, gamma, subsample, colsample_bytree, rate_drop, skip_drop, min_child_weight),\n",
    "  # xgbLinear=expand.grid(nrounds, lambda, alpha, eta),\n",
    "  # xgbTree=expand.grid(nrounds, max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample),\n",
    "  mlpML=expand.grid(layer1=c(50,100),layer2=c(50,100),layer3=c(50,100))\n",
    "  # mlpKerasDropout=expand.grid(size=c(50,100), dropout=seq(0, .7, length=3), batch_size=floor(length(tr_ind)/3), lr=c(2e-6, 2e-3,.1,.5), rho=c(.2,.5,.9), decay=c(0,.3), activation=c(\"relu\",\"softmax\",\"linear\")),\n",
    "  # mlpKerasDecay=expand.grid(size=c(50,100), lambda=seq(0, .7, length=3), batch_size=floor(length(tr_ind)/3), lrlr=c(2e-6, 2e-3,.1,.5), rho=c(.2,.5,.9), decay=c(0,.3), activation=c(\"relu\",\"softmax\",\"linear\"))\n",
    ")\n",
    "\n",
    "# use lapply/loop to run everything; best RMSE chosen by default\n",
    "for (model in models) {\n",
    "  cat(\"\\n\", model, \" ------------------------------------------\");\n",
    "  for (xi in names(m0s)) {\n",
    "    m0 = m0s[[xi]]\n",
    "    mtr = m0[tr_ind,]\n",
    "    \n",
    "    dir.create(paste0(model_dir,\"/\",xi), showWarnings=F)\n",
    "    fname = paste0(model_dir,\"/\",xi,\"/\",model,\".Rdata\")\n",
    "    if (!file.exists(fname) | overwrite) { try ({ cat(\"\\n\", xi)\n",
    "      t2i = NULL\n",
    "      if (model%in%names(pars)) {\n",
    "        t2i = caret::train(y=ctr, x=mtr, model, trControl=fitcv, tuneGrid=pars[[model]])\n",
    "      } else {\n",
    "        t2i = caret::train(y=ctr, x=mtr, model, trControl=fitcv)#, tuneGrid=pars[[model]])\n",
    "      }\n",
    "      if (!is.null(t2i)) save(t2i, file=fname)\n",
    "    }) }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "## 3) load models -----------------------------------\n",
    "feat_dirs = list.dirs(model_dir, full.names=F)\n",
    "feat_dirs = feat_dirs[!feat_dirs%in%\"\"]\n",
    "result0 = llply(feat_dirs, function(data_type) { \n",
    "  models = gsub(\".Rdata\",\"\",list.files(paste0(model_dir,\"/\",data_type)))\n",
    "  a = llply(models, function(model) \n",
    "    get(load(paste0(model_dir,\"/\", data_type,\"/\",model,\".Rdata\"))) )\n",
    "  names(a) = models\n",
    "  return(a)\n",
    "})\n",
    "names(result0) = feat_dirs\n",
    "\n",
    "# cat(\"min rmse's\\n\")\n",
    "result = unlist(result0,recursive=F)\n",
    "# r2 = llply(1:length(result), function(i) {\n",
    "#   cat(models[i],\": \",  round(result[[i]]$results$RMSE[which.min(result[[i]]$results$RMSE)],4),\"\\t\",  result[[i]]$times$everything[3],\"\\n\")\n",
    "# })\n",
    "\n",
    "# results to data frame\n",
    "df1 = ldply (names(result), function(i) {\n",
    "  fm = str_split(i,\"[.]\")[[1]]\n",
    "  data.frame(\n",
    "    rmse=result[[i]]$results$RMSE[which.min(result[[i]]$results$RMSE)],\n",
    "    time=as.numeric(result[[i]]$times$everything[3]),\n",
    "    model_=result[[i]]$modelInfo$label, \n",
    "    feature=fm[1], model=fm[2], \n",
    "    par=paste0( paste0(names(result[[i]]$bestTune), collapse=\"_\"), \": \", paste0(result[[i]]$bestTune, collapse=\"_\") )\n",
    "    , stringsAsFactors=F)\n",
    "})\n",
    "# df1\n",
    "write.table(df1, file=paste0(result_dir,\"/rmse_train.csv\"))\n",
    "\n",
    "## 4) print all results as prediction plots -----------------------\n",
    "# result = unlist(result0, recursive=F)\n",
    "# scores = ldply(names(result), function(xi) {\n",
    "#   x = result[[xi]]\n",
    "#   score = laply(x, function(xi) xi$rmse)\n",
    "#   return(data.frame(feature.model=rep(xi,length(scores)),\n",
    "#                     rmse=score, cv=1:length(scores)))\n",
    "# })\n",
    "# \n",
    "# png(paste0(result_dir, \"/10cv.png\"))\n",
    "# pl = barchart(rmse~feature.model, data=scores, groups=cv, \n",
    "#               auto.key=list(columns=2),\n",
    "#               cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), \n",
    "#               main=\"rmse scores over 10-fold cv's\")\n",
    "# print(pl)\n",
    "# dev.off()\n",
    "\n",
    "\n",
    "## 4) get test prediction results from models ----------------------\n",
    "preds0 = llply(names(result0), function(xi) {\n",
    "  m0 = m0s[[xi]]\n",
    "  res = extractPrediction(result0[[xi]], \n",
    "    testX=m0[te_ind,], testY=cte, unkX=m0[-tr_ind0,]) # some features don't have test data\n",
    "  return(res)\n",
    "}, .parallel=T)\n",
    "names(preds0) = names(result0)\n",
    "save(preds0,file=paste0(result_dir,\"/preds.Rdata\"))\n",
    "\n",
    "load(paste0(result_dir,\"/preds.Rdata\"))\n",
    "wth = 200\n",
    "dir.create(paste0(result_dir,\"/obsVSpred\"), showWarnings=F)\n",
    "for (pred0n in names(preds0)) {\n",
    "  # plot graph to compare models\n",
    "  png(paste0(result_dir,\"/obsVSpred/\",pred0n,\".png\"), \n",
    "      width=wth*length(levels(preds0[[pred0n]]$model)))\n",
    "  pl = plotObsVsPred(preds0[[pred0n]])\n",
    "  print(pl)\n",
    "  graphics.off()\n",
    "  \n",
    "  # png(paste0(result_dir,\"/\",xi,\"_rmse.png\"))\n",
    "  # dotplot(caret::resamples(preds0[[xi]]))\n",
    "  # graphics.off()\n",
    "}\n",
    "\n",
    "\n",
    "## 5) get rmse results and plot ------------------------------------\n",
    "# preds = unlist(preds0,recursive=F)\n",
    "rmse = function(x,y) sqrt(mean((x-y)^2))\n",
    "rmsedf = ldply(names(preds0), function(xi) {\n",
    "  x = preds0[[xi]]\n",
    "  ldply(unique(x$model), function(mi) {\n",
    "    mii = x$model==mi\n",
    "    pr = x$pred[mii]\n",
    "    data.frame(rmse=c(rmse(pr[1:length(tr_ind)], ctr),\n",
    "                      rmse(pr[(length(tr_ind)+1):(length(tr_ind)+length(te_ind))], cte),\n",
    "                      rmse(pr[1:length(tr_ind0)], meta$GA[append(tr_ind,te_ind)])),\n",
    "               feature=rep(xi,3), model=rep(mi,3), type=c(\"train\",\"test\",\"all\"))\n",
    "  })\n",
    "})\n",
    "write.csv(rmsedf, file=paste0(result_dir,\"/rmse.csv\"))\n",
    "\n",
    "wth = 2000\n",
    "png(paste0(result_dir,\"/rmse_all.png\"), width=wth)\n",
    "par(mfrow=c(3,1))\n",
    "pl = barchart(rmse~model, data=rmsedf[rmsedf$type==\"all\",,drop=F], groups=feature, \n",
    "              auto.key = list(columns=2),\n",
    "              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), \n",
    "              main=\"all rmse for each model grouped by feature type\")\n",
    "print(pl)\n",
    "graphics.off()\n",
    "png(paste0(result_dir,\"/rmse_test.png\"), width=wth)\n",
    "pl = barchart(rmse~model, data=rmsedf[rmsedf$type==\"test\",,drop=F], groups=feature, \n",
    "              auto.key = list(columns=2),\n",
    "              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), \n",
    "              main=\"test\")\n",
    "print(pl)\n",
    "graphics.off()\n",
    "png(paste0(result_dir,\"/rmse_train.png\"), width=wth)\n",
    "pl = barchart(rmse~model, data=rmsedf[rmsedf$type==\"train\",,drop=F], groups=feature, \n",
    "              auto.key = list(columns=2),\n",
    "              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), \n",
    "              main=\"train\")\n",
    "print(pl)\n",
    "graphics.off()\n",
    "\n",
    "\n",
    "## 6) check for outlier subjects ------------------------------------\n",
    "trte_diff = ldply(names(preds0), function(xi) {\n",
    "  x = preds0[[xi]]\n",
    "  xdf = ldply(unique(x$model), function(mi) {\n",
    "    mii = x$model==mi\n",
    "    pr = x$pred[mii]\n",
    "    ob = x$pred[mii]\n",
    "    mdf = data.frame(diff=c(pr[1:length(tr_ind0)] - meta$GA[append(tr_ind,te_ind)]), feat.model=paste0(rep(xi,length(tr_ind0)), \".\", rep(mi,length(tr_ind0))), sample=meta$SampleID[append(tr_ind,te_ind)])\n",
    "    return(mdf)\n",
    "  })\n",
    "  return(xdf)\n",
    "})\n",
    "\n",
    "png(paste0(result_dir,\"/outliers.png\"), width=5000)\n",
    "pl = barchart(abs(diff)~sample, data=trte_diff[order(trte_diff$diff),], groups=feat.model, \n",
    "              auto.key = list(columns=5),\n",
    "              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), \n",
    "              main=\"all abs(pred-obs) for each sample grouped by feature.model type\")\n",
    "print(pl)\n",
    "graphics.off()\n",
    "\n",
    "# xdiffs = ldply(names(result), function(xi) {\n",
    "#   xdiff = unlist(llply(result[[xi]], function(x) x$pte)) - meta$GA[tr_ind]\n",
    "#   return(data.frame(feature.model=rep(xi,length(xdiff)),\n",
    "#                     GAdiff=xdiff, sample=meta$SampleID[tr_ind]))\n",
    "# })\n",
    "# \n",
    "# png(paste0(result_dir, \"/10cv_sample.png\"))\n",
    "# par(mfrow=c(1,2))\n",
    "# pl = barchart(GAdiff~sample, data=xdiffs, groups=feature.model, \n",
    "#               auto.key=list(columns=2),\n",
    "#               cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), \n",
    "#               main=\"prediction - ground truth, over sample\")\n",
    "# print(pl)\n",
    "# pl = barchart(abs(GAdiff)~sample, data=xdiffs, groups=feature.model, \n",
    "#               auto.key=list(columns=2),\n",
    "#               cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), \n",
    "#               main=\"absolute prediction - ground truth, over sample\")\n",
    "# print(pl)\n",
    "# dev.off()\n",
    "\n",
    "\n",
    "## 7) save final results of one model/feature ------------------\n",
    "xi = \"features_raw\"\n",
    "model = \"enet\"\n",
    "# finalsol = llply(preds, function(x) x$pred[is.na(x$obs)])\n",
    "class_final$GA = round(preds0[[xi]]$pred[is.na(preds0[[xi]]$obs) & preds0[[xi]]$model==model],1)\n",
    "write.csv(class_final, file=paste0(result_dir,\"/TeamX_SC1_prediction.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
