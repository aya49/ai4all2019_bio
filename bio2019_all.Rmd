---
title: "Invent the Future"
output: html_notebook
---

# Todo:
- Write down better introduction 
- Fix packages
- add text about importance of feature extraction
- explain RMSE
- select models

# input:
- features: (samples x 32830/925032 gene/probeset) RNAseq counts (+extracted features); data has been batch and count normalized
- class: (367 train sample) gestational age 8-42 weeks

# output:
- class: (368 test sample) gestational age 8-42 weeks rounded to 1 decimal place

# Data Analysis 

##1. Preprocessing  

In the first part we will work on the environment we will use. This means we will clean the environment, declare the directory of the project and install libraries.  



```{r, warning=FALSE, message=FALSE, error = FALSE  , results = 'hide' }
rm(list=ls(all=T)) # clean the environment
set.seed(10)


## Defining the work directory of the project  
#root = "/mnt/f/Brinkman group/current/Alice/ai4all2019_bio"
root = "C:\\Users\\raoki\\Documents\\GitHub\\ai4all2019_bio"
setwd(root)


## Creating sub directories to save data, features, model and results 
input_dir = paste0(root,"/00_input") # raw data directory
feat_dir = paste0(root,"/01_features") # feature directory
model_dir = paste0(root, "/02_models") # model directory
result_dir = paste0(root, "/03_results") # stats/plots directory
sapply(c(input_dir,feat_dir, model_dir, result_dir), 
       function(x) dir.create(x, showWarnings=F))

## load packages; need to fix according to what model we'll be using
pkgs = c("Rfast", "stringr", "plyr", "dplyr", "Matrix", # var, str_, llply, etc
         "lattice", "ggplot2", "grid","gridExtra", # barplot, plots
         "foreach", "doMC", # parallel back-end
         "caret", "e1071", "ranger", "ANN2", "randomForest",
         "elasticnet", "fastICA", "foba", "glmnet","kernlab", 
         "KRLS", "lars", "leaps", "nnls", "nodeHarvest", 
         "partDSA", "pls", "plsRglm", "rpart", "rqPen",
         "RSNNS", "spikeslab", "xgboost") # ml
pkgs_ui = setdiff(pkgs, rownames(installed.packages()))
if (length(pkgs_ui) > 0) install.packages(pkgs_ui, verbose=F)
sapply(pkgs, require, character.only=T)

## script options
#no_cores = detectCores()-1 # number of cores to use in parallel
#registerDoMC(no_cores)
#overwrite = F # overwrite results?

```

Next, we will load the input files. There are 3 files:

1. Sample Annotation file: 

* SampleID: unique identifier of the sample (matching the name of the .CEL file in HTA20 folder, except for extension .CEL);
* GA: gestational age as determined by the last menstrual period and or ultrasound;
* Batch: the batch identifier;
* Set: name of the source dataset;
* Train: 1 for samples to be used for training, 0 for samples to be used for test;

2. RNASEQ Data: each row is a gene and each column a patient 

*  probeset: gene ID
*  pid: patient id 

3. Submission template for the competition

```{r,warning=FALSE, message=FALSE, error = FALSE}
## load input files

#Sample Annotation file
meta=  read.csv(paste0(input_dir,"/anoSC1_v11_nokey.csv"))

# RNASEQ data
data0 = t(get(load(paste0(input_dir,"/HTA20_RMA.RData"))))


# submission template
submission = read.csv(paste0(input_dir,"/TeamX_SC1_prediction.csv")) 

```


Data Exploration: 

```{r}
cat('Range\n');range(data0) ; cat('Genes IDs\n');gid = colnames(data0); head(gid) ;cat('Patients IDs\n');pid = rownames(data0); head(pid)
cat('Meta Data head and shape\n'); head(meta); dim(meta)
cat('RNASEQ head and shape\n'); head(data0[,c(1:5)]); dim(data0)
cat('Submission head and shape\n'); head(submission); dim(submission)

```

Now we will explore the data with some plots. First we need to prepare the data and summarize the informatoin we will use in the plots. 


```{r}
# plot stats: mean count, pearson/spearman corr

#Using only the train dataset to calculate the correlation, mean and variance 
data1 = data.frame('SampleID' = rownames(data0), data0)
data1 = merge(meta[,c(1,2,5)],data1,by.x = 'SampleID',by.y = 'SampleID', all = T)

#splitting training set and testing set 
data1 = subset(data1, Train == 1) 
data1 = subset(data1, select = -c(Train,SampleID))

#train set 
data_cor = data.frame(col = names(data1),corr_p = apply(data1, 2, cor, x = data1$GA, method = 'pearson'))
data_cor = data.frame(data_cor, corr_s = apply(data1, 2, cor, x = data1$GA, method = 'spearman'))
rownames(data_cor) = NULL 


#Dataset with all information 
data_cor = data.frame(data_cor, variance = c(var(data1$GA),colVars(data0)), mean = c(mean(data1$GA),colMeans(data0))) 
data_cor = data_cor[-1,] # removing GA from the dataset



```

To make the plots, we will use a library called ggplot2. Here are shown some plots we can make using this library. 

```{r}
# plot stats
p1 <- ggplot(data_cor, aes(x=mean)) + 
  geom_histogram(fill = 'lightgreen') + 
  xlab('Average Expression') + labs(title='(a)')

p2 <- ggplot(data_cor, aes(x=mean, y = variance)) + 
  geom_point(color = 'lightgreen')+
  xlab('Average Expression') + 
  ylab('Variance')+ labs(title='(b)')

p3 <- ggplot(data_cor, aes(x=corr_p)) + 
  geom_histogram(fill = 'lightgreen')+
  xlab('Pearson Correlation between Genes and Gestacional Age')+ labs(title='(c)')

p4 <- ggplot(data_cor, aes(x=corr_p, y = corr_s)) + 
  geom_point(color = 'lightgreen')+ labs(title='(d)')+
  xlab('Pearson Correlation') + ylab('Spearman Correlation')

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

# Exercise: 
1. What can we conclude from these plots above? 
2. Why we don't have the correlation between the gene expression and GA for the testing set? 
3. Can you think in any other intesresting plots or analysis? 

Answer (delete): 
1. a) The average expression is around 5. There are a few genes that are more expressed. b) There is no strong correlation between variance and average gene expression. c) The person correlation between the genes and the Gestacional Age looks like a normal distributio with mean equals 0. This means that most of genes will have a association equals 0. d) The correlation between the Spearman Correlation and Pearson correlation is very strong, meaning that we can use either Spearman or Pearson that the results will be very similar. 
2. We don't make these correlation because we don't know the GA for the testing set. In fact, this is the exacly information that we are looking for. 

#2. Features Extraction 

The original dataset has 32830 genes expressions. However, from the plot of the correlation we already know that some of these genes aren't associate with GA. Besides, genes with a very low variance might don't contribute for the prediction of gestacional age. Also, large datasets might add noise to machine leanring models. 


ADD MORE INFO HERE ABOUT IMPORTANCE OF FEATURE EXTTRACTION


We will use 5 methods do extract features. 
1. Elimination of 30% of genes with lowest variance; 
2. Elimination of 30% of genes with lowest correlation with GA; 
3. PCA
4. Random Forest 
5. Autoenconder 

```{r}

## 1 and 2) Removing genes with low variance and low absolute correlation
data_cor$corr_p = abs(data_cor$corr_p)
keep = subset(data_cor, variance>quantile(data_cor$variance, 0.3) & corr_p>quantile(data_cor$corr_p,0.3))

data2 = subset(data0, select = keep$col)

## Saving for future use
write.csv(data2, paste0(feat_dir,'/features_raw.csv'), row.names=T)


## 3) PCA
PrePCA = preProcess(data2,method="pca")
feat.pca = predict(PrePCA,data2)
write.csv(feat.pca, paste0(feat_dir,'/features_pca.csv'), row.names=T)

## 4) Random Forest --------------------------------------------
metric = "Accuracy"
GA = data1$GA
data1 = subset(data1, select =keep$col)
data1 = data.frame(GA, data1)
PreRF = caret::train(y=data1$GA, x=data1[,-1],  method='ranger',importance='impurity')
PreRF.i = varImp(PreRF)$importance

feat.ra = data2[,order(PreRF.i, decreasing=T)[1:500]]
write.csv(feat.ra, paste0(feat_dir,'/features_ra.csv'), row.names = T)

## 5) Autoencoder --------------------------------------------
preA = autoencoder(data2, hidden.layers = c(1000, 500, 1000))
feat.A = encode(preA, data2)
rownames(feat.A) = rownames(feat.ra)
write.csv(feat.A, paste0(feat_dir,'/features_a.csv'), row.names = T)

#save.image()
```


#3. Machine Learning models

In this section we will work on the prediction task. 
For this, we will test several machine learning models to see which one produced the lowest error. 
We will compare which set of features also produce the best results. 

We will test X models: 
* enet: Fits Elastic Net Regression Models
* foba: Greedy variable selection for ridge regression (?)
* gaussprPolyg: Gaussian Process with Polynomial Kernel
* gaussprRadial: Gaussian Process with Radial Basis Function Kernel
* glmnet: Elastic net model paths for some generalized linear models
* icr: linear regression model using independent components
* kernelpls: Fits a PLSR(Partial Least Squares Regression) model with the kernel algorithm.
* krlsRadial:
* lars2      
* lasso: regression least absolute shrinkage and selection operator
* leapBackward: regression  
* leapForward: regression 
* leapSeq: regression 
* nnls: regression, non-negative least squares
* null
* partDSA: Partitioning Using Deletion, Substitution, and Addition Moves
* pls: Partial Least Squares Regression      
* plsRglm: Partial Least Squares Regression    
* rbf: Radial Basis Function 
* rpart: Recursive Partitioning And Regression Trees
* rqlasso: Ridge Regression and the Lasso
* rvmPoly: Least squares support vector machines poly kernel
* rvmRadial: regression vector machine radial kernel
* simpls: PLSR model with the SIMPLS algorithm
* spikeslab:Prediction and variable selection using spike and slab regression
* spls: Sparse Partial Least Squares 
* svmPoly: Support Vector Machines with Polynomial Kernel 
* svmRadial: Support Vector Machines with Radial 
* svmRadialCost: Support Vector Machines with Radial
* svmRadialSigma: Support Vector Machines with kernel
* widekernelpls: no info
* rqnc: no info 
* nodeHarvest: tree-like estimator
* mlpML: Multilayer Perceptron
* xgbDART



In this chunk, we will load the features that we pre-processed. 
The advange is that if something happen with the code and we need to restart, we don't need to wait for
the code to run the features again, we can just load the results here. 
```{r}
## 0) load features
feat_paths = list.files(feat_dir) # feature paths
features = llply(feat_paths, 
            function(xi) {
              feat = read.csv(paste0(feat_dir,"/", xi))
              rownames(feat) = feat[,1]
              feat = as.matrix(feat[,-1])
              }
            )
names(features) = gsub(".csv","",feat_paths)

```



Explain better here what this part is doing
```{r}
## 1) prep cvn-fold cross validation & rmse function
cvinds_path = paste0(root,"/cvinds.Rdata")
if(file.exists(cvinds_path)){
  load(cvinds_path)
} else {
  cvn = 10
  tr_ind0 = which(meta$Train==1) #selecting only the training examples
  te_ind = sample(tr_ind0, ceiling(length(tr_ind0)/11)) #cross validation set
  tr_ind = sample(tr_ind0[!tr_ind0%in%te_ind]) #removing cross validation set from training set 
  ctr = as.numeric(meta$GA[tr_ind]) 
  cte = as.numeric(meta$GA[te_ind])
  save(cvn,tr_ind0,te_ind,tr_ind,ctr,cte, file=cvinds_path)
}
```



The code bellow shows the names of all models we will be considering. 

```{r}
fitcv = trainControl(method="cv", number=cvn)


## 2) test regression models ---------------------------------

# list models to test
models = c(
  "enet", # 8.5      
  "foba", # 8.5      
  "gaussprPoly", # 8.3
  "gaussprRadial", # 8.8
  "glmnet", # 8.5
  "icr", # 8.4
  "kernelpls", # 8.5
  "krlsRadial", # 8.5
  "lars2", # 8.5      
  "lasso", # 8.7     
  "leapBackward", "leapForward", "leapSeq", # 8.5
  "nnls", # 8.5
  "null", # 8.5
  "partDSA", # 8.5
  "pls",        
  "plsRglm",    
  "rbf", # 8.4       
  "rpart", # 8.4     
  "rqlasso", # 8.4
  "rvmPoly",    "rvmRadial", # 8.5
  "simpls", "spikeslab",  # "spls", # 8.5; spls takes a bit longer 950
  "svmPoly",    "svmRadial",  "svmRadialCost","svmRadialSigma", # 8.5
  "widekernelpls", # 8.5
  "rqnc", # 8.4
  "nodeHarvest", # 8.6
  "mlpML", # 8.5
  "xgbDART" # extreme gradient boosting is good; "xgbLinear", # takes too long
)

# list model parameters to test
pars = list(
  enet=expand.grid(lambda=10^runif(5, min=-5, 1), fraction=runif(5, min=0, max=1)),
  mlpML=expand.grid(layer1=c(50,100),layer2=c(50,100),layer3=c(50,100))
)

overwrite = FALSE
# use lapply/loop to run everything; best RMSE chosen by default
for (model in models) {
  cat("\n", model, " ------------------------------------------");
  for (xi in names(features)) {
    current_features = features[[xi]]
    train = current_features[tr_ind,]
    #mtr = train
    dir.create(paste0(model_dir,"/",xi), showWarnings=F)
    fname = paste0(model_dir,"/",xi,"/",model,".Rdata")
    if (!file.exists(fname) | overwrite) { try ({ cat("\n", xi)
      t2i = NULL
      if (model%in%names(pars)) {
        t2i = caret::train(y=ctr, x=train, model, trControl=fitcv, tuneGrid=pars[[model]])
      } else {
        t2i = caret::train(y=ctr, x=train, model, trControl=fitcv)#, tuneGrid=pars[[model]])
      }
      if (!is.null(t2i)) save(t2i, file=fname)
    }) }
  }
}


```


Instead of run everything, we can also load the results of the models. 
```{r}
## 3) load models -----------------------------------
feat_dirs = list.dirs(model_dir, full.names=F)
feat_dirs = feat_dirs[!feat_dirs%in%""]
result0 = llply(feat_dirs, function(data_type) { 
  models = gsub(".Rdata","",list.files(paste0(model_dir,"/",data_type)))
  a = llply(models, function(model) 
    get(load(paste0(model_dir,"/", data_type,"/",model,".Rdata"))) )
  names(a) = models
  return(a)
})
names(result0) = feat_dirs

```


We will compare the models using RMSE. Explain RMSE

```{r}
result = unlist(result0,recursive=F)


# results to data frame
df1 = ldply (names(result), function(i) {
  fm = str_split(i,"[.]")[[1]]
  data.frame(
    rmse=result[[i]]$results$RMSE[which.min(result[[i]]$results$RMSE)],
    time=as.numeric(result[[i]]$times$everything[3]),
    model_=result[[i]]$modelInfo$label, 
    feature=fm[1], model=fm[2], 
    par=paste0( paste0(names(result[[i]]$bestTune), collapse="_"), ": ", paste0(result[[i]]$bestTune, collapse="_") )
    , stringsAsFactors=F)
})

write.table(df1, file=paste0(result_dir,"/rmse_train.csv"))
```

Now we will construct some graphics to evaluate which models produce the best results. 

```{r}
#Checking best set of features after remove RMSE bigger than 11
df2 = subset(df1, rmse<11)

#Organizing the plot
df2$feature = as.character(df2$feature)
df2$feature[df2$feature=='features_a']= 'Autoenconder'
df2$feature[df2$feature=='features_ra']= 'Random Forest'
df2$feature[df2$feature=='features_pca']= 'PCA'
df2$feature[df2$feature=='features_raw']= 'Raw'


barplot = data.frame(tapply(df2$rmse, df2$feature, mean))
barplot = data.frame(Features = rownames(barplot), barplot)
rownames(barplot) = NULL; names(barplot)[2] = "AverageRMSE"
barplot$AverageRMSE = round(barplot$AverageRMSE,2)

p5 <- ggplot(data = df1[df1$rmse<100,], aes(x=log(rmse))) + geom_histogram(fill = 'lightgreen') + 
  xlab('log(RMSE)') + labs(title='(e)')

p6 <- ggplot(data = df2, aes(x=feature,y=rmse)) + geom_boxplot(fill= 'lightgreen')+
  xlab('Features')+ labs(title='(f)')


p7 <- ggplot(data = barplot, aes(x=Features, y = AverageRMSE))+
  geom_bar(stat = 'identity', fill='lightgreen')+
  ylab('Average RMSE')+ labs(title='(g)')+
  geom_text(aes(label=AverageRMSE), vjust=1.6, color="white", size=3.5)

df1 = df1[order(df1$rmse,decreasing = FALSE),]
df3 = df1[1:20,]

p8<- ggplot(data = df3, aes(x=rmse)) + geom_histogram(fill = 'lightgreen') + 
  xlab('RMSE') + labs(title='(g)')+xlab('RMSE top 20 models')
p8

grid.arrange(p5,p6,p7,p8, nrow = 2)

```
Exploring top 20 models
```{r}
top20 = data.frame(table(df3$model_))
names(top20) = c('Model','Frequency')
top20 = top20[order(top20$Frequency,decreasing = T),]
top20 
```



```{r}

## 4) get test prediction results from models ----------------------
preds0 = llply(names(result0), function(xi) {
  m0 = m0s[[xi]]
  res = extractPrediction(result0[[xi]], 
    testX=m0[te_ind,], testY=cte, unkX=m0[-tr_ind0,]) # some features don't have test data
  return(res)
}, .parallel=T)
names(preds0) = names(result0)
save(preds0,file=paste0(result_dir,"/preds.Rdata"))

load(paste0(result_dir,"/preds.Rdata"))
wth = 200
dir.create(paste0(result_dir,"/obsVSpred"), showWarnings=F)
for (pred0n in names(preds0)) {
  # plot graph to compare models
  png(paste0(result_dir,"/obsVSpred/",pred0n,".png"), 
      width=wth*length(levels(preds0[[pred0n]]$model)))
  pl = plotObsVsPred(preds0[[pred0n]])
  print(pl)
  graphics.off()
  
  # png(paste0(result_dir,"/",xi,"_rmse.png"))
  # dotplot(caret::resamples(preds0[[xi]]))
  # graphics.off()
}


## 5) get rmse results and plot ------------------------------------
# preds = unlist(preds0,recursive=F)
rmse = function(x,y) sqrt(mean((x-y)^2))
rmsedf = ldply(names(preds0), function(xi) {
  x = preds0[[xi]]
  ldply(unique(x$model), function(mi) {
    mii = x$model==mi
    pr = x$pred[mii]
    data.frame(rmse=c(rmse(pr[1:length(tr_ind)], ctr),
                      rmse(pr[(length(tr_ind)+1):(length(tr_ind)+length(te_ind))], cte),
                      rmse(pr[1:length(tr_ind0)], meta$GA[append(tr_ind,te_ind)])),
               feature=rep(xi,3), model=rep(mi,3), type=c("train","test","all"))
  })
})
write.csv(rmsedf, file=paste0(result_dir,"/rmse.csv"))

wth = 2000
png(paste0(result_dir,"/rmse_all.png"), width=wth)
par(mfrow=c(3,1))
pl = barchart(rmse~model, data=rmsedf[rmsedf$type=="all",,drop=F], groups=feature, 
              auto.key = list(columns=2),
              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), 
              main="all rmse for each model grouped by feature type")
print(pl)
graphics.off()
png(paste0(result_dir,"/rmse_test.png"), width=wth)
pl = barchart(rmse~model, data=rmsedf[rmsedf$type=="test",,drop=F], groups=feature, 
              auto.key = list(columns=2),
              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), 
              main="test")
print(pl)
graphics.off()
png(paste0(result_dir,"/rmse_train.png"), width=wth)
pl = barchart(rmse~model, data=rmsedf[rmsedf$type=="train",,drop=F], groups=feature, 
              auto.key = list(columns=2),
              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), 
              main="train")
print(pl)
graphics.off()


## 6) check for outlier subjects ------------------------------------
trte_diff = ldply(names(preds0), function(xi) {
  x = preds0[[xi]]
  xdf = ldply(unique(x$model), function(mi) {
    mii = x$model==mi
    pr = x$pred[mii]
    ob = x$pred[mii]
    mdf = data.frame(diff=c(pr[1:length(tr_ind0)] - meta$GA[append(tr_ind,te_ind)]), feat.model=paste0(rep(xi,length(tr_ind0)), ".", rep(mi,length(tr_ind0))), sample=meta$SampleID[append(tr_ind,te_ind)])
    return(mdf)
  })
  return(xdf)
})

png(paste0(result_dir,"/outliers.png"), width=5000)
pl = barchart(abs(diff)~sample, data=trte_diff[order(trte_diff$diff),], groups=feat.model, 
              auto.key = list(columns=5),
              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), 
              main="all abs(pred-obs) for each sample grouped by feature.model type")
print(pl)
graphics.off()



## 7) save final results of one model/feature ------------------
xi = "features_raw"
model = "enet"
# finalsol = llply(preds, function(x) x$pred[is.na(x$obs)])
submission$GA = round(preds0[[xi]]$pred[is.na(preds0[[xi]]$obs) & preds0[[xi]]$model==model],1)
write.csv(submission, file=paste0(result_dir,"/TeamX_SC1_prediction.csv"))


```

