---
title: "Invent the Future"
output: html_notebook
---

# Todo:
- Write down better introduction 
- Fix packages
- add text about importance of feature extraction
- explain RMSE
- select models

Alice:  Check which packages are still being used 


# input:
- features: (samples x 32830/925032 gene/probeset) RNAseq counts (+extracted features); data has been batch and count normalized
- class: (367 train sample) gestational age 8-42 weeks

# output:
- class: (368 test sample) gestational age 8-42 weeks rounded to 1 decimal place

# Data Analysis 

##1. Preprocessing  

In the first part we will work on the environment we will use. This means we will clean the environment, declare the directory of the project and install libraries.  



```{r, warning=FALSE, message=FALSE, error = FALSE  , results = 'hide' }
rm(list=ls(all=T)) # clean the environment
set.seed(10)


## Defining the work directory of the project  
#root = "/mnt/f/Brinkman group/current/Alice/ai4all2019_bio"
root = "C:\\Users\\raoki\\Documents\\GitHub\\ai4all2019_bio"
setwd(root)


## Creating sub directories to save data, features, model and results 
input_dir = paste0(root,"/00_input") # raw data directory
feat_dir = paste0(root,"/01_features") # feature directory
model_dir = paste0(root, "/02_models") # model directory
result_dir = paste0(root, "/03_results") # stats/plots directory
sapply(c(input_dir,feat_dir, model_dir, result_dir), 
       function(x) dir.create(x, showWarnings=F))

## load packages; need to fix according to what model we'll be using
pkgs = c("Rfast", "stringr", "plyr", "dplyr", "Matrix", # var, str_, llply, etc
         "lattice", "ggplot2", # barplot, plots
         "foreach", "doMC", # parallel back-end
         "caret", "e1071", "ranger", "ANN2", "randomForest",
         "elasticnet", "fastICA", "foba", "glmnet","kernlab", 
         "KRLS", "lars", "leaps", "nnls", "nodeHarvest", 
         "partDSA", "pls", "plsRglm", "rpart", "rqPen",
         "RSNNS", "spikeslab", "xgboost",'Metrics') # ml
pkgs_ui = setdiff(pkgs, rownames(installed.packages()))
if (length(pkgs_ui) > 0) install.packages(pkgs_ui, verbose=F)
sapply(pkgs, require, character.only=T)

## script options
#no_cores = detectCores()-1 # number of cores to use in parallel
#registerDoMC(no_cores)
overwrite = F 


#load workspace for time consuming tasks 
load(paste(input_dir,'script.RData',sep='\\'))

```

Next, we will load the input files. There are 3 files:

1. Sample Annotation file: 

* SampleID: unique identifier of the sample (matching the name of the .CEL file in HTA20 folder, except for extension .CEL);
* GA: gestational age as determined by the last menstrual period and or ultrasound;
* Batch: the batch identifier;
* Set: name of the source dataset;
* Train: 1 for samples to be used for training, 0 for samples to be used for test;

2. RNASEQ Data: each row is a gene and each column a patient 

*  probeset: gene ID
*  pid: patient id 

3. Submission template for the competition

```{r,warning=FALSE, message=FALSE, error = FALSE}
## load input files

#Sample Annotation file
meta=  read.csv(paste0(input_dir,"/anoSC1_v11_nokey.csv"))

# RNASEQ data
data0 = t(get(load(paste0(input_dir,"/HTA20_RMA.RData"))))


# submission template
submission = read.csv(paste0(input_dir,"/TeamX_SC1_prediction.csv")) 

```


Data Exploration: 

```{r}
cat('Range\n');range(data0) ; cat('Genes IDs\n');gid = colnames(data0); head(gid) ;cat('Patients IDs\n');pid = rownames(data0); head(pid)
cat('Meta Data head and shape\n'); head(meta); dim(meta)
cat('RNASEQ head and shape\n'); head(data0[,c(1:5)]); dim(data0)
cat('Submission head and shape\n'); head(submission); dim(submission)

```

Now we will explore the data with some plots. First we need to prepare the data and summarize the informatoin we will use in the plots. 


```{r}
# plot stats: mean count, pearson/spearman corr

#Using only the train dataset to calculate the correlation, mean and variance 
data1 = data.frame('SampleID' = rownames(data0), data0)
data1 = merge(meta[,c(1,2,5)],data1,by.x = 'SampleID',by.y = 'SampleID', all = T)

#splitting training set and testing set 
data1 = subset(data1, Train == 1) 
data1 = subset(data1, select = -c(Train,SampleID))

#train set 
data_cor = data.frame(col = names(data1),corr_p = apply(data1, 2, cor, x = data1$GA, method = 'pearson'))
data_cor = data.frame(data_cor, corr_s = apply(data1, 2, cor, x = data1$GA, method = 'spearman'))
rownames(data_cor) = NULL 


#Dataset with all information 
data_cor = data.frame(data_cor, variance = c(var(data1$GA),colVars(data0)), mean = c(mean(data1$GA),colMeans(data0))) 
data_cor = data_cor[-1,] # removing GA from the dataset



```

To make the plots, we will use a library called ggplot2. Here are shown some plots we can make using this library. 

```{r}
# plot stats
p1 <- ggplot(data_cor, aes(x=mean)) + 
  geom_histogram(fill = 'lightgreen') + 
  xlab('Average Expression') + labs(title='(a)')

p2 <- ggplot(data_cor, aes(x=mean, y = variance)) + 
  geom_point(color = 'lightgreen')+
  xlab('Average Expression') + 
  ylab('Variance')+ labs(title='(b)')

p3 <- ggplot(data_cor, aes(x=corr_p)) + 
  geom_histogram(fill = 'lightgreen')+
  xlab('Pearson Correlation between Genes and Gestacional Age')+ labs(title='(c)')

p4 <- ggplot(data_cor, aes(x=corr_p, y = corr_s)) + 
  geom_point(color = 'lightgreen')+ labs(title='(d)')+
  xlab('Pearson Correlation') + ylab('Spearman Correlation')

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

# Exercise 1: 
1. What can we conclude from these plots above? 
2. Why we don't have the correlation between the gene expression and GA for the testing set? 
3. Can you think in any other intesresting plots or analysis? 

```{r}
#insert here a new plot or anaylysis you think is instering

```



Answer (delete): 
1. a) The average expression is around 5. There are a few genes that are more expressed. b) There is no strong correlation between variance and average gene expression. c) The person correlation between the genes and the Gestacional Age looks like a normal distributio with mean equals 0. This means that most of genes will have a association equals 0. d) The correlation between the Spearman Correlation and Pearson correlation is very strong, meaning that we can use either Spearman or Pearson that the results will be very similar. 
2. We don't make these correlation because we don't know the GA for the testing set. In fact, this is the exacly information that we are looking for. 

#2. Features Extraction 

The original dataset has 32830 genes expressions. However, from the plot of the correlation we already know that some of these genes aren't associate with GA. Besides, genes with a very low variance might don't contribute for the prediction of gestacional age. Also, large datasets might add noise to machine leanring models. 


ADD MORE INFO HERE ABOUT IMPORTANCE OF FEATURE EXTTRACTION


We will use 5 methods do extract features. 
1. Elimination of 30% of genes with lowest variance; 
2. Elimination of 30% of genes with lowest correlation with GA; 
3. PCA
4. Random Forest 
5. Autoenconder 

```{r, results='hide'}

## 1 and 2) Removing genes with low variance and low absolute correlation
data_cor$corr_p = abs(data_cor$corr_p)
keep = subset(data_cor, variance>quantile(data_cor$variance, 0.3) & corr_p>quantile(data_cor$corr_p,0.3))

data2 = subset(data0, select = keep$col)

## Saving for future use
write.csv(data2, paste0(feat_dir,'/features_raw.csv'), row.names=T)

```

```{r}
## 3) PCA
PrePCA = preProcess(data2,method="pca")
feat.pca = predict(PrePCA,data2)
write.csv(feat.pca, paste0(feat_dir,'/features_pca.csv'), row.names=T)

```

```{r}
## 4) Random Forest 
metric = "Accuracy"
GA = data1$GA
data1 = subset(data1, select =keep$col)
data1 = data.frame(GA, data1)
if(!exists('PreRF')){
  PreRF = caret::train(y=data1$GA, x=data1[,-1],  method='ranger',importance='impurity')
  PreRF.i = varImp(PreRF)$importance
}else{
  PreRF.i = varImp(PreRF)$importance
}

feat.ra = data2[,order(PreRF.i, decreasing=T)[1:500]]
write.csv(feat.ra, paste0(feat_dir,'/features_ra.csv'), row.names = T)

```


```{r}
## 5) Autoencoder 
if(!exists('preA')){
  preA = autoencoder(data2, hidden.layers = c(1000, 500, 1000))
  feat.A = encode(preA, data2)  
}
rownames(feat.A) = rownames(feat.ra)
write.csv(feat.A, paste0(feat_dir,'/features_a.csv'), row.names = T)
```

### Exercise 2: 
1. What parameters do you think you could change? 
2. Choose between PCA and Random Forest. Try to save a new features set with a different number of features. 

```{r}
#feat.unique = predict(PrePCA,data2) #for pca OR
#feat.unique = data2[] #for random forest
#write.csv(feat.unique, )

```


#3. Machine Learning models

In this section we will work which the Machine Learning Models. We have 5 different sets of features (RAW, PCA, RA, A, UNIQUE) and we will first work with 3 models: Linear Regression, RVM and Random Forest. The best model will be selected based on the difference between the values we observed on the training set for GA (Gestacional Age) and the predicted values for the combination of MODEL+FEATURE. The lowest is this difference on the validation set, the better. 


ALICE: Explain better here what this part is doing
```{r}
## 1) prep cvn-fold cross validation & rmse function
cvinds_path = paste0(root,"/cvinds.Rdata")
if(file.exists(cvinds_path)){
  load(cvinds_path)
} else {
  cvn = 10
  train_index = which(meta$Train==1) #selecting only the training examples
  train_index_val = sample(train_index, ceiling(length(train_index)/11)) #cross validation set
  train_index_ = sample(train_index[!train_index%in%train_index_val]) #removing cross validation set from training set 
  ga_val = as.numeric(meta$GA[train_index_val])
  ga_ = as.numeric(meta$GA[train_index_]) 
  save(cvn,train_index,train_index_val,train_index_,ga_val,ga_, file=cvinds_path)
}

require(Metrics)

```


##3.1 Linear Regression 

ALICE: ADD SHORT DESCRIPTION 

```{r, warning=FALSE}
feature_type = 'pca' #options are 'pca', 'raw', 'ra' for random forest and 'a' for autoencoder
## 0) load feature 
features = read.csv(paste(feat_dir,"/features_",feature_type,'.csv', sep = ''))
rownames(features) = features[,1]
features = as.matrix(features[,-1])

features_ = features[train_index_,]
features_val = features[train_index_val,]

train_ = data.frame(ga_,features_)
model1 = lm(ga_ ~ .,data = train_)
predictions_m1 = predict(model1, newdata = train_[,-1])

rmse(ga_, predictions_m1)

```
```{r}
plotdata = data.frame(ga_,predictions_m1)
ggplot(plotdata, aes(x = ga_,y = predictions_m1))+
  geom_point(color = 'lightgreen')+
  xlab('Linear Regression Model - Training Set')+
  ylab('Predicted Values')

```

### Exercise 3: 
Calculate the predictions for features_val and ga_val using linear regression. With the predicted values, make a plot predictions_m1_val and ga_val
```{r, warning=FALSE}
#predictions_m1_val = predict()
#rmse()
```
```{r}
#plotdata = data.frame()
#ggplot(plotdata, aes(x = ,y = ))+geom_point(color = '')+xlab('')+ylab('')

```



##3.2 RVM

ALICE: ADD SHORT DESCRIPTION

```{r}

model2 = rvm(x = features_, ga_, type="regression")
predictions_m2 = predict(model2,data = as.data.frame(features_))
rmse(ga_,predictions_m2)


plotdata = data.frame(ga_,predictions_m2)
ggplot(plotdata, aes(x = ga_,y = predictions_m2))+
  geom_point(color = 'lightgreen')+
  ylab('Predicted Values')+
  xlab('RVM - Training Set')

```
### Exercise 4: 
Calculate the predictions for features_val and ga_val using rvm. With the predicted values, make a plot predictions_m1_val and ga_val
```{r, warning=FALSE}
#predictions_m2_val = predict()
#rmse()
```


```{r}
#plotdata = data.frame()
#ggplot(plotdata, aes(x = ,y = ))+geom_point(color = '')+xlab('')+ylab('')

```


##3.3 Random Forest

ALICE: ADD SHORT DESCRIPTION

```{r}
model3 = ranger(ga_~.,data = train_)
predictions_m3 = predict(model3, data = as.data.frame(features_)) 
rmse(ga_,predictions_m3$predictions)

plotdata = data.frame(ga_,predictions_m3$predictions)
ggplot(plotdata, aes(x = ga_,y = predictions_m3.predictions))+
  geom_point(color = 'lightgreen')+
  ylab('Predicted Values')+
  xlab('Random Forest - Training Set')

```
### Exercise 5: 
Calculate the predictions for features_val and ga_val using Random Forest. With the predicted values, make a plot predictions_m1_val and ga_val
```{r, warning=FALSE}
#predictions_m3_val = predict()
#rmse()
```


```{r}
#plotdata = 
#ggplot(plotdata, aes(x = ,y = ))+geom_point(color = '')+xlab('')+ylab('')

```


### Exercise 6:
Repite the Linear Regression, SCM and Random Forest with 2 other sets of features (options are 'raw', 'ra' for random forest, 'a' for autoencoder or 'unique' from  Exercise 2). From the set of features you choose, which one have the best result? Why?

SET OF FEATURES 1
```{r, warning=FALSE}
#feature_type =  #options are 'pca', 'raw', 'ra' for random forest and 'a' for autoencoder
## 0) load feature 
features = read.csv(paste(feat_dir,"/features_",feature_type,'.csv', sep = ''))
rownames(features) = features[,1]
features = as.matrix(features[,-1])

features_ = features[train_index_,]
features_val = features[train_index_val,]

train_ = data.frame(ga_,features_)

#Linear Regression
model1 = lm(ga_ ~ .,data = train_)
predictions_m1 = predict(model1, newdata = train_[,-1])

rmse(ga_, predictions_m1)

```
```{r}
plotdata = data.frame(ga_,predictions_m1)
ggplot(plotdata, aes(x = ga_,y = predictions_m1))+
  geom_point(color = 'lightgreen')+
  xlab('Linear Regression Model - Training Set')+
  ylab('Predicted Values')



ALICE: Explain RMSE


#RAQUEL STOPPED HERE, CODE BELLOW NOT ORGANIZED
The code bellow shows the names of all models we will be considering. 

```{r}
# overwrite model?
overwrite = F

# feature
xi = "features_raw" # ra, a, pca

# model
model = "enet"

# list model parameters to test; see https://topepo.github.io/caret/available-models.html
parsi = expand.grid(lambda=10^runif(5, min=-5, 1), fraction=runif(5, min=0, max=1)) # parsi = NULL; if no parameters need to be tested



fitcv = trainControl(method="cv", number=cvn)


```



```{r}
## 2) test regression models ---------------------------------

# best RMSE parameters chosen by default
mtr = m0[tr_ind,]
dir.create(paste0(model_dir,"/",xi), showWarnings=F)
fname = paste0(model_dir,"/",xi,"/",model,".Rdata")
if (!file.exists(fname) | overwrite) { try ({
  t2i = NULL
  if (!is.null(parsi)) {
    t2i = caret::train(y=ctr, x=mtr, model, trControl=fitcv, tuneGrid=parsi)
  } else {
    t2i = caret::train(y=ctr, x=mtr, model, trControl=fitcv)
  }
  if (!is.null(t2i)) save(t2i, file=fname)
}) }
load(fname)

# results to data frame
df = data.frame(
  rmse=t2i$results$RMSE[which.min(t2i$results$RMSE)],
  time=as.numeric(t2i$times$everything[3]),
  model_=t2i$modelInfo$label, 
  feature=xi, model=model, 
  par=paste0( paste0(names(t2i$bestTune), collapse="_"), ": ", paste0(t2i$bestTune, collapse="_") )
  , stringsAsFactors=F)
df

```



```{r}
## 4) get test prediction results from models ----------------------
pred = extractPrediction(t2i, testX=m0[te_ind,], testY=cte, unkX=m0[-tr_ind0,])

# plot graph to compare models
wth = 200
dir.create(paste0(result_dir,"/obsVSpred"), showWarnings=F)
png(paste0(result_dir,"/obsVSpred/",xi,"_",model,".png"), width=wth)
pl = plotObsVsPred(pred)
print(pl)
graphics.off()


## 5) get rmse results and plot ------------------------------------
# preds = unlist(preds0,recursive=F)
rmse = function(x,y) sqrt(mean((x-y)^2))
rdf = ldply(unique(pred$model), function(mi) {
  mii = pred$model==mi
  pr = pred$pred[mii]
  data.frame(rmse=c(rmse(pr[1:length(tr_ind)], ctr),
                    rmse(pr[(length(tr_ind)+1):(length(tr_ind)+length(te_ind))], cte),
                    rmse(pr[1:length(tr_ind0)], meta$GA[append(tr_ind,te_ind)])),
             feature=rep(xi,3), model=rep(mi,3), type=c("train","test","all"))
})
rdf



## 7) save final results of one model/feature ------------------

# submission template
class_final = read.csv(paste0(input_dir,"/TeamX_SC1_prediction.csv")) 

class_final$GA = round(pred$pred[is.na(pred$obs) & pred$model==model],1)
write.csv(class_final, file=paste0(result_dir,"/TeamX_SC1_prediction.csv"))
```


Instead of run everything, we can also load the results of the models. 
```{r}
## 3) load models -----------------------------------
feat_dirs = list.dirs(model_dir, full.names=F)
feat_dirs = feat_dirs[!feat_dirs%in%""]
result0 = llply(feat_dirs, function(data_type) { 
  models = gsub(".Rdata","",list.files(paste0(model_dir,"/",data_type)))
  a = llply(models, function(model) 
    get(load(paste0(model_dir,"/", data_type,"/",model,".Rdata"))) )
  names(a) = models
  return(a)
})
names(result0) = feat_dirs

```


We will compare the models using RMSE. Explain RMSE

```{r}
result = unlist(result0,recursive=F)


# results to data frame
df1 = ldply (names(result), function(i) {
  fm = str_split(i,"[.]")[[1]]
  data.frame(
    rmse=result[[i]]$results$RMSE[which.min(result[[i]]$results$RMSE)],
    time=as.numeric(result[[i]]$times$everything[3]),
    model_=result[[i]]$modelInfo$label, 
    feature=fm[1], model=fm[2], 
    par=paste0( paste0(names(result[[i]]$bestTune), collapse="_"), ": ", paste0(result[[i]]$bestTune, collapse="_") )
    , stringsAsFactors=F)
})

write.table(df1, file=paste0(result_dir,"/rmse_train.csv"))
```

Now we will construct some graphics to evaluate which models produce the best results. 

```{r}
#Checking best set of features after remove RMSE bigger than 11
df2 = subset(df1, rmse<11)

#Organizing the plot
df2$feature = as.character(df2$feature)
df2$feature[df2$feature=='features_a']= 'Autoenconder'
df2$feature[df2$feature=='features_ra']= 'Random Forest'
df2$feature[df2$feature=='features_pca']= 'PCA'
df2$feature[df2$feature=='features_raw']= 'Raw'


barplot = data.frame(tapply(df2$rmse, df2$feature, mean))
barplot = data.frame(Features = rownames(barplot), barplot)
rownames(barplot) = NULL; names(barplot)[2] = "AverageRMSE"
barplot$AverageRMSE = round(barplot$AverageRMSE,2)

p5 <- ggplot(data = df1[df1$rmse<100,], aes(x=log(rmse))) + geom_histogram(fill = 'lightgreen') + 
  xlab('log(RMSE)') + labs(title='(e)')

p6 <- ggplot(data = df2, aes(x=feature,y=rmse)) + geom_boxplot(fill= 'lightgreen')+
  xlab('Features')+ labs(title='(f)')


p7 <- ggplot(data = barplot, aes(x=Features, y = AverageRMSE))+
  geom_bar(stat = 'identity', fill='lightgreen')+
  ylab('Average RMSE')+ labs(title='(g)')+
  geom_text(aes(label=AverageRMSE), vjust=1.6, color="white", size=3.5)

df1 = df1[order(df1$rmse,decreasing = FALSE),]
df3 = df1[1:20,]

p8<- ggplot(data = df3, aes(x=rmse)) + geom_histogram(fill = 'lightgreen') + 
  xlab('RMSE') + labs(title='(g)')+xlab('RMSE top 20 models')

grid.arrange(p5,p6,p7,p8, nrow = 2)

```
Exploring top 20 models
```{r}
top20 = data.frame(table(df3$model_))
names(top20) = c('Model','Frequency')
top20 = top20[order(top20$Frequency,decreasing = T),]
top20 
```

```{r}
## 0) load features

feat_paths = list.files(feat_dir) # feature paths
features = llply(feat_paths, 
            function(xi) {
              feat = read.csv(paste0(feat_dir,"/", xi))
              rownames(feat) = feat[,1]
              feat = as.matrix(feat[,-1])
              }
            )
names(features) = gsub(".csv","",feat_paths)

```


```{r}

## 4) get test prediction results from models ----------------------
preds0 = llply(names(result0), function(xi) {
  m0 = m0s[[xi]]
  res = extractPrediction(result0[[xi]], 
    testX=m0[te_ind,], testY=cte, unkX=m0[-tr_ind0,]) # some features don't have test data
  return(res)
}, .parallel=T)
names(preds0) = names(result0)
save(preds0,file=paste0(result_dir,"/preds.Rdata"))

load(paste0(result_dir,"/preds.Rdata"))
wth = 200
dir.create(paste0(result_dir,"/obsVSpred"), showWarnings=F)
for (pred0n in names(preds0)) {
  # plot graph to compare models
  png(paste0(result_dir,"/obsVSpred/",pred0n,".png"), 
      width=wth*length(levels(preds0[[pred0n]]$model)))
  pl = plotObsVsPred(preds0[[pred0n]])
  print(pl)
  graphics.off()
  
  # png(paste0(result_dir,"/",xi,"_rmse.png"))
  # dotplot(caret::resamples(preds0[[xi]]))
  # graphics.off()
}


## 5) get rmse results and plot ------------------------------------
# preds = unlist(preds0,recursive=F)
rmse = function(x,y) sqrt(mean((x-y)^2))
rmsedf = ldply(names(preds0), function(xi) {
  x = preds0[[xi]]
  ldply(unique(x$model), function(mi) {
    mii = x$model==mi
    pr = x$pred[mii]
    data.frame(rmse=c(rmse(pr[1:length(tr_ind)], ctr),
                      rmse(pr[(length(tr_ind)+1):(length(tr_ind)+length(te_ind))], cte),
                      rmse(pr[1:length(tr_ind0)], meta$GA[append(tr_ind,te_ind)])),
               feature=rep(xi,3), model=rep(mi,3), type=c("train","test","all"))
  })
})
write.csv(rmsedf, file=paste0(result_dir,"/rmse.csv"))

wth = 2000
png(paste0(result_dir,"/rmse_all.png"), width=wth)
par(mfrow=c(3,1))
pl = barchart(rmse~model, data=rmsedf[rmsedf$type=="all",,drop=F], groups=feature, 
              auto.key = list(columns=2),
              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), 
              main="all rmse for each model grouped by feature type")
print(pl)
graphics.off()
png(paste0(result_dir,"/rmse_test.png"), width=wth)
pl = barchart(rmse~model, data=rmsedf[rmsedf$type=="test",,drop=F], groups=feature, 
              auto.key = list(columns=2),
              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), 
              main="test")
print(pl)
graphics.off()
png(paste0(result_dir,"/rmse_train.png"), width=wth)
pl = barchart(rmse~model, data=rmsedf[rmsedf$type=="train",,drop=F], groups=feature, 
              auto.key = list(columns=2),
              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), 
              main="train")
print(pl)
graphics.off()


## 6) check for outlier subjects ------------------------------------
trte_diff = ldply(names(preds0), function(xi) {
  x = preds0[[xi]]
  xdf = ldply(unique(x$model), function(mi) {
    mii = x$model==mi
    pr = x$pred[mii]
    ob = x$pred[mii]
    mdf = data.frame(diff=c(pr[1:length(tr_ind0)] - meta$GA[append(tr_ind,te_ind)]), feat.model=paste0(rep(xi,length(tr_ind0)), ".", rep(mi,length(tr_ind0))), sample=meta$SampleID[append(tr_ind,te_ind)])
    return(mdf)
  })
  return(xdf)
})

png(paste0(result_dir,"/outliers.png"), width=5000)
pl = barchart(abs(diff)~sample, data=trte_diff[order(trte_diff$diff),], groups=feat.model, 
              auto.key = list(columns=5),
              cex.axis=3, scales=list(x=list(rot=90,cex=0.8)), 
              main="all abs(pred-obs) for each sample grouped by feature.model type")
print(pl)
graphics.off()



## 7) save final results of one model/feature ------------------
xi = "features_raw"
model = "enet"
# finalsol = llply(preds, function(x) x$pred[is.na(x$obs)])
submission$GA = round(preds0[[xi]]$pred[is.na(preds0[[xi]]$obs) & preds0[[xi]]$model==model],1)
write.csv(submission, file=paste0(result_dir,"/TeamX_SC1_prediction.csv"))


```



All models available: 
* enet: Fits Elastic Net Regression Models
* foba: Greedy variable selection for ridge regression (?)
* gaussprPolyg: Gaussian Process with Polynomial Kernel
* gaussprRadial: Gaussian Process with Radial Basis Function Kernel
* glmnet: Elastic net model paths for some generalized linear models
* icr: linear regression model using independent components
* kernelpls: Fits a PLSR(Partial Least Squares Regression) model with the kernel algorithm.
* krlsRadial:
* lars2      
* lasso: regression least absolute shrinkage and selection operator
* leapBackward: regression  
* leapForward: regression 
* leapSeq: regression 
* nnls: regression, non-negative least squares
* null
* partDSA: Partitioning Using Deletion, Substitution, and Addition Moves
* pls: Partial Least Squares Regression      
* plsRglm: Partial Least Squares Regression    
* rbf: Radial Basis Function 
* rpart: Recursive Partitioning And Regression Trees
* rqlasso: Ridge Regression and the Lasso
* rvmPoly: Least squares support vector machines poly kernel
* rvmRadial: regression vector machine radial kernel
* simpls: PLSR model with the SIMPLS algorithm
* spikeslab:Prediction and variable selection using spike and slab regression
* spls: Sparse Partial Least Squares 
* svmPoly: Support Vector Machines with Polynomial Kernel 
* svmRadial: Support Vector Machines with Radial 
* svmRadialCost: Support Vector Machines with Radial
* svmRadialSigma: Support Vector Machines with kernel
* widekernelpls: no info
* rqnc: no info 
* nodeHarvest: tree-like estimator
* mlpML: Multilayer Perceptron
* xgbDART